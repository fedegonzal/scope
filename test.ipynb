{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-18 13:00:38.063939: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-18 13:00:38.064028: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-18 13:00:38.066142: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-18 13:00:38.078304: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-18 13:00:39.487839: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/notebooks/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/notebooks/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/notebooks/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5011 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "  1%|          | 46/5011 [05:49<10:29:29,  7.61s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 138\u001b[0m\n\u001b[1;32m    126\u001b[0m sum_atts_resized_norm \u001b[38;5;241m=\u001b[39m (sum_atts_resized \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(sum_atts_resized)) \u001b[38;5;241m/\u001b[39m (np\u001b[38;5;241m.\u001b[39mmax(sum_atts_resized) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(sum_atts_resized))\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m#print(\"sum_atts_resized_norm\", sum_atts_resized_norm.min(), sum_atts_resized_norm.max())\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m#plt.imshow(sum_atts_resized_norm)\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m#plt.show()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m \n\u001b[1;32m    137\u001b[0m \u001b[38;5;66;03m# Get the depth prediction\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m depth_feats, depth_image \u001b[38;5;241m=\u001b[39m \u001b[43mget_depth_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpil_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdepth_checkpoint\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# Obtain the depth map\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Resize depth_image to the original image size\u001b[39;00m\n\u001b[1;32m    143\u001b[0m numpy_depth_image \u001b[38;5;241m=\u001b[39m depth_image\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/notebooks/myutils/depth.py:16\u001b[0m, in \u001b[0;36mget_depth_prediction\u001b[0;34m(pil_img, model)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 16\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdepth_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     predicted_depth \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpredicted_depth\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# interpolate to original size\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/dpt/modeling_dpt.py:1137\u001b[0m, in \u001b[0;36mDPTForDepthEstimation.forward\u001b[0;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     backbone_hidden_states\u001b[38;5;241m.\u001b[39mextend(\n\u001b[1;32m   1132\u001b[0m         feature \u001b[38;5;28;01mfor\u001b[39;00m idx, feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(hidden_states[\u001b[38;5;241m1\u001b[39m:]) \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mbackbone_out_indices[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m   1133\u001b[0m     )\n\u001b[1;32m   1135\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m backbone_hidden_states\n\u001b[0;32m-> 1137\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1139\u001b[0m predicted_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(hidden_states)\n\u001b[1;32m   1141\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/dpt/modeling_dpt.py:993\u001b[0m, in \u001b[0;36mDPTNeck.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of hidden states should be equal to the number of neck hidden sizes.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    992\u001b[0m \u001b[38;5;66;03m# postprocess hidden states\u001b[39;00m\n\u001b[0;32m--> 993\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreassemble_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    995\u001b[0m features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs[i](feature) \u001b[38;5;28;01mfor\u001b[39;00m i, feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(features)]\n\u001b[1;32m    997\u001b[0m \u001b[38;5;66;03m# fusion blocks\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/dpt/modeling_dpt.py:644\u001b[0m, in \u001b[0;36mDPTReassembleStage.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    642\u001b[0m readout \u001b[38;5;241m=\u001b[39m cls_token\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand_as(hidden_state)\n\u001b[1;32m    643\u001b[0m \u001b[38;5;66;03m# concatenate the readout token to the hidden states and project\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadout_projects\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreadout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;66;03m# reshape back to (B, C, H, W)\u001b[39;00m\n\u001b[1;32m    646\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m hidden_state\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(feature_shape)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/activations.py:78\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 1. read an image\n",
    "# 2. convert to tensor and transform\n",
    "# 3. padding to prepare for dino model\n",
    "\n",
    "# 6. pass through the model to get the depth\n",
    "# 7. obtain the depth map\n",
    "\n",
    "# 4. pass through dino model to get features\n",
    "# 5. obtain the attention map\n",
    "\n",
    "# 8. process depth (get the attention)\n",
    "# 9. obtain the attention map from depth processed\n",
    "# 10. process the attention map\n",
    "# 11. obtain the final attention map merged from depth and dino\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from myutils.depth import *\n",
    "from myutils.dino1 import load_dino1_model\n",
    "from myutils.dino2 import load_dino2_model\n",
    "from myutils.discovery import *\n",
    "from myutils.pascal_voc import *\n",
    "from myutils.ssl import *\n",
    "from myutils.utils import *\n",
    "from myutils.datasets import bbox_iou\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import  DPTForDepthEstimation, DPTImageProcessor\n",
    "\n",
    "import base64\n",
    "import json\n",
    "\n",
    "# dino v1\n",
    "params = {\n",
    "    'patch_size': 16,\n",
    "    'ssl_checkpoint': 'pretrained/dino_deitsmall16_pretrain.pth',\n",
    "    'depth_checkpoint': 'Intel/dpt-hybrid-midas',\n",
    "    'img_size': None\n",
    "}\n",
    "\n",
    "# dino v2\n",
    "#params = {\n",
    "#    'patch_size': 14,\n",
    "#    'ssl_checkpoint': 'pretrained/dinov2_vits14_reg4_pretrain.pth',\n",
    "#    'depth_checkpoint': 'Intel/dpt-beit-base-384',\n",
    "#    'img_size': 526\n",
    "#}\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Get N random images from dataset_path\n",
    "# n=None to get all images\n",
    "dataset_path = \"datasets/VOC2007/VOCdevkit/VOC2007\"\n",
    "images = get_images(dataset_path, -1)\n",
    "\n",
    "#images = ['005881', '000250', '001855', '007586', '009079', '003159', '009745', '004801', '004244', '006185', '002405', '000334', '008349', '007853', '001263', '007193', '008856', '003678', '008833', '000109', '002873', '009881', '007040', '007751', '008917', '003369', '003629', '004828', '004192', '008415', '008733', '009418', '006694', '005676', '006339', '001590', '004281', '008096', '009438', '006212', '005064', '008372', '009343', '001185', '009307', '003433', '002266', '009214', '009421', '007417', '000951', '004359', '005732', '007697', '001981', '003147', '002224', '003051', '004706', '000047', '001393', '001510', '002977', '003994', '002880', '005790', '006247', '006282', '004441', '006822', '006626', '007398', '007261', '009246', '007503', '001343', '001250', '003807', '000667', '003120', '000489', '007372', '001455', '000470', '004797', '005101', '000917', '005312', '007847', '009904', '006425', '000483', '001927', '009331', '004518', '000173', '002439', '005365', '007621', '006968']\n",
    "#images = ['009331', '002439', '005365', '007621', '006968']\n",
    "\n",
    "corloc = np.zeros(len(images))\n",
    "\n",
    "print(\"Processing images...\")\n",
    "\n",
    "# let's use tqdm to show a progress bar\n",
    "for i, img_name in enumerate(tqdm(images)):\n",
    "\n",
    "    image_path = f'{dataset_path}/JPEGImages/{img_name}.jpg'\n",
    "    annotation_path = f'{dataset_path}/Annotations/{img_name}.xml'\n",
    "\n",
    "\n",
    "    ##############\n",
    "    # WARMING UP #\n",
    "    ##############\n",
    "\n",
    "    # Read an image\n",
    "    pil_img = load_image_as_pil(image_path)\n",
    "\n",
    "    # Convert to tensor and transform\n",
    "    img_tensor = load_image_as_tensor(pil_img).to(device)\n",
    "\n",
    "    # Get the ground truth\n",
    "    ground_truth, ground_truth_img = get_ground_truth_voc2007(annotation_path, pil_img)\n",
    "\n",
    "    #plt.imshow(ground_truth_img)\n",
    "    #plt.show()\n",
    "\n",
    "    #########################\n",
    "    # GETTING THE ATTENTION #\n",
    "    #########################\n",
    "\n",
    "    # Padding the image with zeros to fit multiple of patch-size\n",
    "    patch_size = params['patch_size']\n",
    "    img_paded = img_tensor_padded(img_tensor, patch_size).to(device)\n",
    "\n",
    "    # Load the SSL model\n",
    "    model = load_dino1_model(patch_size, params['ssl_checkpoint'], device, params['img_size'])\n",
    "    #model = load_dino2_model(patch_size, params['ssl_checkpoint'], device, params['img_size'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Get the attentions\n",
    "    atts = get_attentions(model, img_paded, patch_size)\n",
    "\n",
    "    # Obtain the attention map\n",
    "\n",
    "    # Sum the attention outputs (6 outputs used in DINO)\n",
    "    sum_atts = atts.sum(0)\n",
    "\n",
    "    noise = estimate_noise(sum_atts)\n",
    "    entropy = calculate_entropy(sum_atts)\n",
    "\n",
    "    # Resize the attentions to the original image size\n",
    "    sum_atts_resized = cv2.resize(sum_atts, (pil_img.width, pil_img.height))\n",
    "\n",
    "    #plt.imshow(sum_atts_resized)\n",
    "    #plt.show()\n",
    "\n",
    "    # Normalize sum_atts_resized from 0 to 1\n",
    "    sum_atts_resized_norm = (sum_atts_resized - np.min(sum_atts_resized)) / (np.max(sum_atts_resized) - np.min(sum_atts_resized))\n",
    "\n",
    "    #print(\"sum_atts_resized_norm\", sum_atts_resized_norm.min(), sum_atts_resized_norm.max())\n",
    "    #plt.imshow(sum_atts_resized_norm)\n",
    "    #plt.show()\n",
    "\n",
    "\n",
    "    #################\n",
    "    # GETTING DEPTH #\n",
    "    #################\n",
    "\n",
    "    # Get the depth prediction\n",
    "    depth_feats, depth_image = get_depth_prediction(pil_img, params['depth_checkpoint'])\n",
    "\n",
    "    # Obtain the depth map\n",
    "\n",
    "    # Resize depth_image to the original image size\n",
    "    numpy_depth_image = depth_image.squeeze(0).cpu().numpy()\n",
    "    depth_image_resized = cv2.resize(numpy_depth_image, pil_img.size)\n",
    "\n",
    "    # plt.imshow(depth_image_resized)\n",
    "    # plt.show()\n",
    "\n",
    "    # Normalizing depth\n",
    "    depth_image_resized_norm = (depth_image_resized - np.min(depth_image_resized)) / (np.max(depth_image_resized) - np.min(depth_image_resized))\n",
    "\n",
    "\n",
    "    ############################\n",
    "    # FINAL ATT * GLOBAL DEPTH #\n",
    "    ############################\n",
    "\n",
    "    # final attention map (depth * att)\n",
    "    final_attention_map = depth_image_resized_norm * sum_atts_resized_norm\n",
    "\n",
    "    #plt.imshow(final_attention_map)\n",
    "    #plt.show()\n",
    "\n",
    "    # Normalize final_attention_map from 0 to 255\n",
    "    final_attention_map_255 = (final_attention_map - np.min(final_attention_map)) / (np.max(final_attention_map) - np.min(final_attention_map)) * 255\n",
    "    final_attention_map_255 = final_attention_map_255.astype(np.uint8)\n",
    "\n",
    "\n",
    "    ########################\n",
    "    # THRESHOLDED & RESULT #\n",
    "    ########################\n",
    "\n",
    "    # Apply a binary threshold to the image\n",
    "    _, final_att_thresholded = cv2.threshold(final_attention_map_255, final_attention_map_255.std(), 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Get the predicted boxes from the final image using contours\n",
    "    predicted_boxes = get_boxes(final_att_thresholded)\n",
    "\n",
    "    #plt.imshow(final_att_thresholded)\n",
    "    #plt.show()\n",
    "\n",
    "\n",
    "    ###################\n",
    "    # DEPTH SPLITTING #\n",
    "    ###################\n",
    "\n",
    "    # Normalize the depth image from 0 to 1\n",
    "    img_to_split_norm = depth_image_resized_norm.copy()\n",
    "\n",
    "    # Define the number of intervals\n",
    "    num_layers = 10\n",
    "\n",
    "    # Compute the ranges dynamically\n",
    "    ranges = [(i / num_layers, (i + 1) / num_layers) for i in range(num_layers)]\n",
    "\n",
    "    # Create an empty list to store the depth_layers for each range\n",
    "    depth_layers = []\n",
    "\n",
    "    # Loop through each range and create the corresponding image\n",
    "    for r in ranges:\n",
    "        lower, upper = r\n",
    "        mask = (img_to_split_norm >= lower) & (img_to_split_norm < upper)\n",
    "        range_img = np.where(mask, img_to_split_norm, 0)  # Retain original values within range, set others to 0\n",
    "        depth_layers.append(range_img)\n",
    "\n",
    "    # let´s remove the background layers (num_layers / 3)\n",
    "    depth_layers = depth_layers[num_layers // 3:]\n",
    "\n",
    "    proposed_boxes = []\n",
    "\n",
    "    for depth_layer in depth_layers:\n",
    "\n",
    "        #normalize depth_layer\n",
    "        depth_layer = (depth_layer - np.min(depth_layer)) / (np.max(depth_layer) - np.min(depth_layer))\n",
    "\n",
    "        #resize depth_layer to att size\n",
    "        depth_layer = cv2.resize(depth_layer, (sum_atts_resized_norm.shape[1], sum_atts_resized_norm.shape[0]))\n",
    "\n",
    "        # multiply depth_layer by att\n",
    "        depth_layer_atts = depth_layer * sum_atts_resized_norm\n",
    "\n",
    "        #plt.imshow(depth_layer_atts)\n",
    "        #plt.show()\n",
    "\n",
    "        # normalize depth_layer_atts from 0 to 255\n",
    "        depth_layer_atts = depth_layer_atts - np.min(depth_layer_atts)\n",
    "        depth_layer_atts = depth_layer_atts / np.max(depth_layer_atts) * 255\n",
    "        depth_layer_atts = depth_layer_atts.astype(np.uint8)\n",
    "\n",
    "        # Apply a binary threshold to the image\n",
    "        _, layer = cv2.threshold(depth_layer_atts, depth_layer_atts.std(), 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        #plt.imshow(layer)\n",
    "        #plt.show()\n",
    "\n",
    "        # Get the predicted boxes from the final image using contours\n",
    "        layer_predicted_boxes = get_boxes(layer)\n",
    "\n",
    "        # Append the predicted boxes to the proposed_boxes\n",
    "        proposed_boxes.extend(layer_predicted_boxes)\n",
    "\n",
    "        # Get the final image with the boxes drawn (predicted and ground truth)\n",
    "\n",
    "        # final_image = get_output_image(pil_img, layer_predicted_boxes, ground_truth)\n",
    "\n",
    "        #plt.imshow(final_image)\n",
    "        #plt.show()\n",
    "\n",
    "\n",
    "    ###############\n",
    "    # FINAL BOXES #\n",
    "    ###############\n",
    "\n",
    "    # Let´s validate the proposed_boxes with predicted_boxes\n",
    "    # if the IoU is greater than iou_threshold, we consider it a match\n",
    "\n",
    "    iou_threshold = 0.1\n",
    "    final_boxes = []\n",
    "\n",
    "    for proposed_box in proposed_boxes:\n",
    "\n",
    "        ious = bbox_iou(torch.tensor(proposed_box), torch.tensor(predicted_boxes), x1y1x2y2=False)\n",
    "\n",
    "        for iou in ious:\n",
    "            if iou > iou_threshold:\n",
    "                final_boxes.append(proposed_box)\n",
    "\n",
    "    # let´s draw the final image with the proposed_boxes matched with the predicted_boxes\n",
    "    final_boxes_image = get_output_image(pil_img, final_boxes, ground_truth)\n",
    "\n",
    "    #plt.imshow(final_boxes_image)\n",
    "    #plt.show()\n",
    "\n",
    "\n",
    "    #################\n",
    "    # LOCAL RESULTS #\n",
    "    #################\n",
    "\n",
    "    ground_truth_as_list = convert_ground_truth_voc2007_to_list(ground_truth)\n",
    "    local_corloc, ious = get_corloc_and_ious(ground_truth_as_list, final_boxes)\n",
    "\n",
    "    # Storing image-corloc to obtain final dataset-corloc later\n",
    "    corloc[i] = local_corloc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "    ############################\n",
    "    # FINAL ATT * GLOBAL DEPTH #\n",
    "    ############################\n",
    "\n",
    "    # final attention map (depth * att)\n",
    "    final_attention_map = depth_image_resized_norm * sum_atts_resized_norm\n",
    "\n",
    "    #plt.imshow(final_attention_map)\n",
    "    #plt.show()\n",
    "\n",
    "    # Normalize final_attention_map from 0 to 255\n",
    "    final_attention_map_255 = (final_attention_map - np.min(final_attention_map)) / (np.max(final_attention_map) - np.min(final_attention_map)) * 255\n",
    "    final_attention_map_255 = final_attention_map_255.astype(np.uint8)\n",
    "\n",
    "\n",
    "    ########################\n",
    "    # THRESHOLDED & RESULT #\n",
    "    ########################\n",
    "\n",
    "    # Apply a binary threshold to the image\n",
    "    _, final_att_thresholded = cv2.threshold(final_attention_map_255, final_attention_map_255.std(), 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Get the predicted boxes from the final image using contours\n",
    "    predicted_boxes = get_boxes(final_att_thresholded)\n",
    "\n",
    "    #plt.imshow(final)\n",
    "    #plt.show()\n",
    "\n",
    "    # Get the final image with the boxes drawn (predicted and ground truth)\n",
    "\n",
    "    final_image = get_output_image(pil_img, predicted_boxes, ground_truth)\n",
    "\n",
    "    #plt.imshow(final_image)\n",
    "    #plt.show()\n",
    "\n",
    "    ground_truth_as_list = convert_ground_truth_voc2007_to_list(ground_truth)\n",
    "    local_corloc, ious = get_corloc_and_ious(ground_truth_as_list, predicted_boxes)\n",
    "\n",
    "    # Storing image-corloc to obtain final dataset-corloc later\n",
    "    corloc[i] = local_corloc\n",
    "\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    ########################\n",
    "    # PLOTTING THE PROCESS #\n",
    "    ########################\n",
    "\n",
    "    # Plot 6 images in a grid of 2x3\n",
    "    title = f\"Image: {img_name} // Corloc: {corloc[i]}\"\n",
    "\n",
    "    plot_image_grid(\n",
    "        title, \n",
    "        pil_img, \n",
    "        depth_image_resized, \n",
    "        sum_atts_resized, \n",
    "        final_attention_map, \n",
    "        final_att_thresholded, \n",
    "        final_boxes_image,\n",
    "        noise,\n",
    "        entropy\n",
    "    )\n",
    "\n",
    "    # Plot the depth_layers for visualization\n",
    "    fig, axes = plt.subplots(1, len(depth_layers), figsize=(24, 5))\n",
    "    for i, (range_img, r) in enumerate(zip(depth_layers, ranges)):\n",
    "        ax = axes[i]\n",
    "        ax.imshow(range_img)\n",
    "        ax.set_title(f'Range {r[0]:.4f} to {r[1]:.4f}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    #plt.show()\n",
    "\n",
    "    # Plot the depth_layers for visualization\n",
    "    fig, axes = plt.subplots(1, len(depth_layers), figsize=(24, 5))\n",
    "    for i, (range_img, r) in enumerate(zip(depth_layers, ranges)):\n",
    "        ax = axes[i]\n",
    "        ax.imshow(range_img * sum_atts_resized_norm)\n",
    "        ax.set_title(f'Att Range {r[0]:.4f} to {r[1]:.4f}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    #plt.show()\n",
    "\n",
    "    # plt.imshow(final_boxes_image)\n",
    "    # plt.show()\n",
    "    '''\n",
    "\n",
    "    ###################\n",
    "    # STORING RESULTS #\n",
    "    ###################\n",
    "\n",
    "    # encode data as base64\n",
    "    ious_b64 = base64.b64encode(json.dumps(ious).encode())\n",
    "    predicted_boxes_b64 = base64.b64encode(json.dumps(predicted_boxes).encode())\n",
    "    ground_truth_b64 = base64.b64encode(json.dumps(ground_truth_as_list).encode())\n",
    "    \n",
    "    #results_file.write(f\"{i},{img_name},{corloc[i]},{ious_b64},{predicted_boxes_b64},{ground_truth_b64}\\n\")\n",
    "\n",
    "\n",
    "############################\n",
    "# FINAL CORLOC CALCULATION #\n",
    "############################\n",
    "\n",
    "# Let's calculate corloc as the percentage of images with at least one IoU > 0.5\n",
    "corloc = corloc.sum() / len(images)\n",
    "\n",
    "print(\"-------------------\")\n",
    "print(\"Final CorLoc: \", corloc)\n",
    "print(\"-------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
